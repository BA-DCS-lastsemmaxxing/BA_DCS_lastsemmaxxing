{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4c37c344-32ac-4405-bc12-5b8c86b5238b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "499fa217dbb8493780554913dd544b16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='.pdf', description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import io\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from pypdf import PdfReader\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean text by removing unwanted characters and formatting.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Remove non-ASCII characters\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters and numbers\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespace\n",
    "    text = re.sub(r'\\n+', '\\n', text)  # Remove extra newlines\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "\n",
    "def preprocess_pdf(file_content, filename):\n",
    "    \"\"\"Extract and preprocess text from uploaded PDF file.\"\"\"\n",
    "    try:\n",
    "        # Convert memoryview to BytesIO\n",
    "        pdf_stream = io.BytesIO(file_content)\n",
    "\n",
    "        reader = PdfReader(pdf_stream)\n",
    "        extracted_text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                extracted_text += text + \"\\n\"\n",
    "\n",
    "        cleaned_text = clean_text(extracted_text)\n",
    "        cleaned_text = remove_stop_words(cleaned_text)\n",
    "\n",
    "        # Save cleaned text to file\n",
    "        output_path = os.path.join(os.getcwd(), f\"{filename}.txt\")\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as text_file:\n",
    "            text_file.write(cleaned_text)\n",
    "\n",
    "        print(f\"Processed text saved to: {output_path}\")\n",
    "        print(f\"Preview:\\n{cleaned_text[:500]}...\")  # Print first 500 chars\n",
    "\n",
    "        # Save metadata for second script\n",
    "        metadata = {\n",
    "            \"filename\": filename,\n",
    "            \"file_path\": output_path\n",
    "        }\n",
    "        with open(\"processed_file.pkl\", \"wb\") as f:\n",
    "            pickle.dump(metadata, f)\n",
    "\n",
    "        print(\"Metadata saved for evaluation script.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF: {e}\")\n",
    "\n",
    "\n",
    "# Upload Widget\n",
    "upload_widget = widgets.FileUpload(\n",
    "    accept='.pdf',  # Accept only PDF files\n",
    "    multiple=False  # Only allow single file upload\n",
    ")\n",
    "\n",
    "\n",
    "def on_upload_change(change):\n",
    "    \"\"\"Handle file upload and process PDF.\"\"\"\n",
    "    if upload_widget.value:\n",
    "        for file_info in upload_widget.value:\n",
    "            filename = file_info['name'].replace(\".pdf\", \"\")\n",
    "            file_content = file_info['content'].tobytes()  # Convert memoryview to bytes\n",
    "            \n",
    "            # Process PDF\n",
    "            preprocess_pdf(file_content, filename)\n",
    "\n",
    "\n",
    "# Attach event listener\n",
    "upload_widget.observe(on_upload_change, names='value')\n",
    "\n",
    "display(upload_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2ee50902-50c3-45e7-9b87-0d36eca64c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "626 Banks_GCO vetted.pdf\n"
     ]
    }
   ],
   "source": [
    "for file_info in upload_widget.value:\n",
    "    print(file_info.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b4d400f9-7951-47e9-80c3-8acf77c2c72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'name': '626 Banks_GCO vetted.pdf', 'type': 'application/pdf', 'size': 166662, 'content': <memory at 0x16803f700>, 'last_modified': datetime.datetime(2025, 1, 26, 19, 6, 56, tzinfo=datetime.timezone.utc)},)\n"
     ]
    }
   ],
   "source": [
    "print(upload_widget.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bcc990fc-2be4-496f-bec1-585a4e0272a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating file: 626 Banks_GCO vetted\n",
      "Predicted Topic: Anti Money Laundering\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"codes.env\")\n",
    "\n",
    "# AWS credentials\n",
    "aws_access_key = os.environ.get(\"AWS_ACCESS_KEY_ID\")\n",
    "aws_secret_key = os.environ.get(\"AWS_SECRET_ACCESS_KEY\")\n",
    "aws_region = os.environ.get(\"AWS_REGION\")\n",
    "\n",
    "# AWS Bedrock model configuration\n",
    "MODEL_ID_LLAMA = \"arn:aws:bedrock:us-west-2:874280117166:inference-profile/us.meta.llama3-3-70b-instruct-v1:0\"\n",
    "\n",
    "# Prevent Bedrock timeout\n",
    "config = Config(read_timeout=1000)\n",
    "\n",
    "client = boto3.client(\n",
    "    \"bedrock-runtime\",\n",
    "    region_name=aws_region,\n",
    "    aws_access_key_id=aws_access_key,\n",
    "    aws_secret_access_key=aws_secret_key,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Load topic mappings\n",
    "mapping_file_path = 'final_file_topic_mapping.csv'\n",
    "file_topic_mapping = pd.read_csv(mapping_file_path)\n",
    "unique_topics = file_topic_mapping['folder_name'].unique().tolist()\n",
    "unique_topics_str = ', '.join(unique_topics)\n",
    "\n",
    "\n",
    "def read_txt_file(file_path):\n",
    "    \"\"\"Reads the content of a .txt file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def map_to_category(predicted_output):\n",
    "    \"\"\"Maps the model's output to a known category.\"\"\"\n",
    "    predicted_output = predicted_output.lower().strip()\n",
    "    for topic in unique_topics:\n",
    "        if topic.lower() in predicted_output:\n",
    "            return topic\n",
    "    return \"unknown\"\n",
    "\n",
    "\n",
    "def evaluate_topic_with_llama(file_content):\n",
    "    \"\"\"Classify the text using AWS Bedrock (Meta's Llama 3.3 70B Instruct).\"\"\"\n",
    "    try:\n",
    "        prompt = f\"Classify the following text into only one of these topics: {unique_topics_str}. \\n{file_content}\"\n",
    "        formatted_prompt = f\"\"\"\n",
    "            <|begin_of_text|>\n",
    "            <|start_header_id|>user<|end_header_id|>\n",
    "            {prompt}\n",
    "            <|eot_id|>\n",
    "            <|start_header_id|>assistant<|end_header_id|>\n",
    "            \"\"\"\n",
    "\n",
    "        response = client.invoke_model(\n",
    "            modelId=MODEL_ID_LLAMA,\n",
    "            body=json.dumps({\n",
    "                \"prompt\": formatted_prompt,\n",
    "                \"max_gen_len\": 512,\n",
    "                \"temperature\": 0,\n",
    "            }),\n",
    "            contentType=\"application/json\"\n",
    "        )\n",
    "        response_body = json.loads(response['body'].read())\n",
    "        predicted_topic = response_body.get(\"generation\", \"\").strip()\n",
    "        \n",
    "        if not predicted_topic:\n",
    "            print(\"Empty response from AWS Bedrock Llama, defaulting to unknown.\")\n",
    "\n",
    "        return map_to_category(predicted_topic)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling AWS Bedrock API: {e}\")\n",
    "        return \"unknown\"\n",
    "\n",
    "\n",
    "def evaluate_saved_file():\n",
    "    \"\"\"Loads metadata, reads file content, and evaluates it.\"\"\"\n",
    "    try:\n",
    "        # Load metadata\n",
    "        with open(\"processed_file.pkl\", \"rb\") as f:\n",
    "            metadata = pickle.load(f)\n",
    "\n",
    "        filename = metadata[\"filename\"]\n",
    "        file_path = metadata[\"file_path\"]\n",
    "\n",
    "        print(f\"Evaluating file: {filename}\")\n",
    "\n",
    "        # Read file content\n",
    "        text_content = read_txt_file(file_path)\n",
    "        if text_content:\n",
    "            predicted_topic = evaluate_topic_with_llama(text_content)\n",
    "            print(f\"Predicted Topic: {predicted_topic}\")\n",
    "        else:\n",
    "            print(\"Error: No content found in the file.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: No processed file metadata found. Run `upload_pdf.py` first.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n",
    "\n",
    "# Run the evaluation\n",
    "evaluate_saved_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8676987-5f55-4652-9dba-ed0c4bafc91f",
   "metadata": {},
   "source": [
    "# showing demo of explanation before predicted topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fb42825e-af5a-41d0-8515-e915ed92563c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating file: 626 Banks_GCO vetted\n",
      "Explanation: Explanation: The document appears to be a set of guidelines issued by the Monetary Authority of Singapore (MAS) to banks and other financial institutions in Singapore, outlining the requirements and procedures for preventing money laundering and countering the financing of terrorism. The guidelines cover various aspects, including customer due diligence, risk-based approach, suspicious transaction reporting, and record-keeping. The document is highly technical and regulatory in nature, indicating that it belongs to the topic of Anti-Money Laundering.\n",
      "\n",
      "        Final Topic: Anti Money Laundering\n",
      "Predicted Topic: Anti Money Laundering\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import re\n",
    "from botocore.config import Config\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"codes.env\")\n",
    "\n",
    "# AWS credentials\n",
    "aws_access_key = os.environ.get(\"AWS_ACCESS_KEY_ID\")\n",
    "aws_secret_key = os.environ.get(\"AWS_SECRET_ACCESS_KEY\")\n",
    "aws_region = os.environ.get(\"AWS_REGION\")\n",
    "\n",
    "# AWS Bedrock model configuration\n",
    "MODEL_ID_LLAMA = \"us.meta.llama3-3-70b-instruct-v1:0\"\n",
    "\n",
    "# Prevent Bedrock timeout\n",
    "config = Config(read_timeout=1000)\n",
    "\n",
    "client = boto3.client(\n",
    "    \"bedrock-runtime\",\n",
    "    region_name=aws_region,\n",
    "    aws_access_key_id=aws_access_key,\n",
    "    aws_secret_access_key=aws_secret_key,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Load topic mappings\n",
    "mapping_file_path = 'final_file_topic_mapping.csv'\n",
    "file_topic_mapping = pd.read_csv(mapping_file_path)\n",
    "unique_topics = file_topic_mapping['folder_name'].unique().tolist()\n",
    "unique_topics_str = ', '.join(unique_topics)\n",
    "\n",
    "def read_txt_file(file_path):\n",
    "    \"\"\"Reads the content of a .txt file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_final_topic(response_text):\n",
    "    \"\"\"\n",
    "    Extracts the final topic from the model's response using regex.\n",
    "    Ensures that we capture the topic stated explicitly at the end.\n",
    "    \"\"\"\n",
    "    match = re.search(r\"Final Topic:\\s*(.+)\", response_text, re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    \n",
    "    # If no clear label is found, fall back to the last line\n",
    "    lines = response_text.strip().split(\"\\n\")\n",
    "    return lines[-1].strip() if lines else \"unknown\"\n",
    "\n",
    "def evaluate_topic_with_llama(file_content):\n",
    "    \"\"\"Classify the text using AWS Bedrock (Meta's Llama 3.3 70B Instruct) with an explanation.\"\"\"\n",
    "    try:\n",
    "        prompt = f\"\"\"\n",
    "        Analyze the following document in depth and determine which topic it belongs to from the given list: {unique_topics_str}. \n",
    "        Provide a justification first, then explicitly state the final topic in the format below:\n",
    "\n",
    "        Explanation: <Your explanation>\n",
    "        Final Topic: <One of the topics from the list>\n",
    "\n",
    "        Text:\n",
    "        {file_content}\n",
    "\n",
    "        Final classification:\n",
    "        \"\"\"\n",
    "\n",
    "        formatted_prompt = f\"\"\"\n",
    "            <|begin_of_text|>\n",
    "            <|start_header_id|>user<|end_header_id|>\n",
    "            {prompt}\n",
    "            <|eot_id|>\n",
    "            <|start_header_id|>assistant<|end_header_id|>\n",
    "            \"\"\"\n",
    "        \n",
    "        response = client.invoke_model(\n",
    "            modelId=MODEL_ID_LLAMA,\n",
    "            body=json.dumps({\n",
    "                \"prompt\": formatted_prompt,\n",
    "                \"max_gen_len\": 512,\n",
    "                \"temperature\": 0,\n",
    "            }),\n",
    "            contentType=\"application/json\"\n",
    "        )\n",
    "\n",
    "        response_body = json.loads(response['body'].read())\n",
    "        response_text = response_body.get(\"generation\", \"\").strip()\n",
    "\n",
    "        if not response_text:\n",
    "            print(\"Empty response from AWS Bedrock Llama, defaulting to unknown.\")\n",
    "            return \"unknown\", \"unknown\"\n",
    "\n",
    "        predicted_topic = extract_final_topic(response_text)\n",
    "        return response_text, predicted_topic  # Return both full response and extracted topic\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error calling AWS Bedrock API: {e}\")\n",
    "        return \"unknown\", \"unknown\"\n",
    "\n",
    "def evaluate_saved_file():\n",
    "    \"\"\"Loads metadata, reads file content, and evaluates it with explanation.\"\"\"\n",
    "    try:\n",
    "        # Load metadata\n",
    "        with open(\"processed_file.pkl\", \"rb\") as f:\n",
    "            metadata = pickle.load(f)\n",
    "\n",
    "        filename = metadata[\"filename\"]\n",
    "        file_path = metadata[\"file_path\"]\n",
    "\n",
    "        print(f\"Evaluating file: {filename}\")\n",
    "\n",
    "        # Read file content\n",
    "        text_content = read_txt_file(file_path)\n",
    "        if text_content:\n",
    "            explanation, predicted_topic = evaluate_topic_with_llama(text_content)\n",
    "            print(f\"Explanation: {explanation}\\nPredicted Topic: {predicted_topic}\")\n",
    "        else:\n",
    "            print(\"Error: No content found in the file.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: No processed file metadata found. Run `upload_pdf.py` first.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n",
    "# Run the evaluation\n",
    "evaluate_saved_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181dfc26-71ba-4fbd-a297-c750740cb41a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
