{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c37c344-32ac-4405-bc12-5b8c86b5238b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brianlau/opt/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7203e94408ab41f8850045ff54391f9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={}, accept='.pdf', description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import io\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "from pypdf import PdfReader\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Clean text by removing unwanted characters and formatting.\n",
    "    \"\"\"\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)  # Remove non-ASCII characters\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters and numbers\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespace\n",
    "    text = re.sub(r'\\n+', '\\n', text)  # Remove extra newlines\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "\n",
    "def preprocess_pdf(file_content, filename):\n",
    "    \"\"\"Extract and preprocess text from uploaded PDF file.\"\"\"\n",
    "    try:\n",
    "        # Convert memoryview to BytesIO\n",
    "        pdf_stream = io.BytesIO(file_content)\n",
    "\n",
    "        reader = PdfReader(pdf_stream)\n",
    "        extracted_text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                extracted_text += text + \"\\n\"\n",
    "\n",
    "        cleaned_text = clean_text(extracted_text)\n",
    "        cleaned_text = remove_stop_words(cleaned_text)\n",
    "\n",
    "        # Save cleaned text to file\n",
    "        output_path = os.path.join(os.getcwd(), f\"{filename}.txt\")\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as text_file:\n",
    "            text_file.write(cleaned_text)\n",
    "\n",
    "        print(f\"Processed text saved to: {output_path}\")\n",
    "        print(f\"Preview:\\n{cleaned_text[:500]}...\")  # Print first 500 chars\n",
    "\n",
    "        # Save metadata for second script\n",
    "        metadata = {\n",
    "            \"filename\": filename,\n",
    "            \"file_path\": output_path\n",
    "        }\n",
    "        with open(\"processed_file.pkl\", \"wb\") as f:\n",
    "            pickle.dump(metadata, f)\n",
    "\n",
    "        print(\"Metadata saved for evaluation script.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing PDF: {e}\")\n",
    "\n",
    "\n",
    "# Upload Widget\n",
    "upload_widget = widgets.FileUpload(\n",
    "    accept='.pdf',  # Accept only PDF files\n",
    "    multiple=False  # Only allow single file upload\n",
    ")\n",
    "\n",
    "\n",
    "def on_upload_change(change):\n",
    "    \"\"\"Handle file upload and process PDF.\"\"\"\n",
    "    if upload_widget.value:\n",
    "        for file_info in upload_widget.value:\n",
    "            filename = file_info['name'].replace(\".pdf\", \"\")\n",
    "            file_content = file_info['content'].tobytes()  # Convert memoryview to bytes\n",
    "            \n",
    "            # Process PDF\n",
    "            preprocess_pdf(file_content, filename)\n",
    "\n",
    "\n",
    "# Attach event listener\n",
    "upload_widget.observe(on_upload_change, names='value')\n",
    "\n",
    "display(upload_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ee50902-50c3-45e7-9b87-0d36eca64c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "626 Banks_GCO vetted.pdf\n"
     ]
    }
   ],
   "source": [
    "for file_info in upload_widget.value:\n",
    "    print(file_info.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4d400f9-7951-47e9-80c3-8acf77c2c72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'name': '626 Banks_GCO vetted.pdf', 'type': 'application/pdf', 'size': 166662, 'content': <memory at 0x1310afac0>, 'last_modified': datetime.datetime(2025, 1, 26, 19, 6, 56, tzinfo=datetime.timezone.utc)},)\n"
     ]
    }
   ],
   "source": [
    "print(upload_widget.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bcc990fc-2be4-496f-bec1-585a4e0272a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating file: 626 Banks_GCO vetted\n",
      "Predicted Topic: Anti Money Laundering\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import pandas as pd\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"codes.env\")\n",
    "\n",
    "# AWS credentials\n",
    "aws_access_key = os.environ.get(\"AWS_ACCESS_KEY_ID\")\n",
    "aws_secret_key = os.environ.get(\"AWS_SECRET_ACCESS_KEY\")\n",
    "aws_region = os.environ.get(\"AWS_REGION\")\n",
    "\n",
    "# AWS Bedrock model configuration\n",
    "MODEL_ID_LLAMA = \"arn:aws:bedrock:us-west-2:874280117166:inference-profile/us.meta.llama3-3-70b-instruct-v1:0\"\n",
    "\n",
    "# Prevent Bedrock timeout\n",
    "config = Config(read_timeout=1000)\n",
    "\n",
    "client = boto3.client(\n",
    "    \"bedrock-runtime\",\n",
    "    region_name=aws_region,\n",
    "    aws_access_key_id=aws_access_key,\n",
    "    aws_secret_access_key=aws_secret_key,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Load topic mappings\n",
    "mapping_file_path = 'final_file_topic_mapping.csv'\n",
    "file_topic_mapping = pd.read_csv(mapping_file_path)\n",
    "unique_topics = file_topic_mapping['folder_name'].unique().tolist()\n",
    "unique_topics_str = ', '.join(unique_topics)\n",
    "\n",
    "\n",
    "def read_txt_file(file_path):\n",
    "    \"\"\"Reads the content of a .txt file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def map_to_category(predicted_output):\n",
    "    \"\"\"Maps the model's output to a known category.\"\"\"\n",
    "    predicted_output = predicted_output.lower().strip()\n",
    "    for topic in unique_topics:\n",
    "        if topic.lower() in predicted_output:\n",
    "            return topic\n",
    "    return \"unknown\"\n",
    "\n",
    "\n",
    "def evaluate_topic_with_llama(file_content):\n",
    "    \"\"\"Classify the text using AWS Bedrock (Meta's Llama 3.3 70B Instruct).\"\"\"\n",
    "    try:\n",
    "        prompt = f\"Classify the following text into only one of these topics: {unique_topics_str}. \\n{file_content}\"\n",
    "        formatted_prompt = f\"\"\"\n",
    "            <|begin_of_text|>\n",
    "            <|start_header_id|>user<|end_header_id|>\n",
    "            {prompt}\n",
    "            <|eot_id|>\n",
    "            <|start_header_id|>assistant<|end_header_id|>\n",
    "            \"\"\"\n",
    "\n",
    "        response = client.invoke_model(\n",
    "            modelId=MODEL_ID_LLAMA,\n",
    "            body=json.dumps({\n",
    "                \"prompt\": formatted_prompt,\n",
    "                \"max_gen_len\": 512,\n",
    "                \"temperature\": 0,\n",
    "            }),\n",
    "            contentType=\"application/json\"\n",
    "        )\n",
    "        response_body = json.loads(response['body'].read())\n",
    "        predicted_topic = response_body.get(\"generation\", \"\").strip()\n",
    "        \n",
    "        if not predicted_topic:\n",
    "            print(\"Empty response from AWS Bedrock Llama, defaulting to unknown.\")\n",
    "\n",
    "        return map_to_category(predicted_topic)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling AWS Bedrock API: {e}\")\n",
    "        return \"unknown\"\n",
    "\n",
    "\n",
    "def evaluate_saved_file():\n",
    "    \"\"\"Loads metadata, reads file content, and evaluates it.\"\"\"\n",
    "    try:\n",
    "        # Load metadata\n",
    "        with open(\"processed_file.pkl\", \"rb\") as f:\n",
    "            metadata = pickle.load(f)\n",
    "\n",
    "        filename = metadata[\"filename\"]\n",
    "        file_path = metadata[\"file_path\"]\n",
    "\n",
    "        print(f\"Evaluating file: {filename}\")\n",
    "\n",
    "        # Read file content\n",
    "        text_content = read_txt_file(file_path)\n",
    "        if text_content:\n",
    "            predicted_topic = evaluate_topic_with_llama(text_content)\n",
    "            print(f\"Predicted Topic: {predicted_topic}\")\n",
    "        else:\n",
    "            print(\"Error: No content found in the file.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"Error: No processed file metadata found. Run `upload_pdf.py` first.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n",
    "\n",
    "# Run the evaluation\n",
    "evaluate_saved_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a66fe6e-f862-4782-a8b4-0f46482b642b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "1c74280073e0e2d1b1a443f530ccd6f3fc15affc33bce9394f7c3af6eb4cd51b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
