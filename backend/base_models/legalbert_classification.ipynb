{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32e9a01b-2987-49a3-aed2-410f72be30aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Running on device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "accb7fc6-e885-4f24-b383-f9e190f475e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "class DataPreparator:\n",
    "    def __init__(self, model_name: str, max_length: int = 256):\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def split_data(self, data, label_col='label', text_col='text', test_size=0.2, random_state=42):\n",
    "        \"\"\"\n",
    "        Splits data into train and validation sets. Avoids stratification if class counts are too low.\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame(data)\n",
    "        stratify_col = None\n",
    "\n",
    "        # Check if stratification is possible\n",
    "        if df[label_col].value_counts().min() > 1:\n",
    "            stratify_col = df[label_col]\n",
    "\n",
    "        train_df, val_df = train_test_split(\n",
    "            df, \n",
    "            test_size=test_size, \n",
    "            random_state=random_state, \n",
    "            stratify=stratify_col\n",
    "        )\n",
    "        return train_df, val_df\n",
    "    def tokenize(self, texts):\n",
    "        \"\"\"\n",
    "        Applies the BERT tokenizer to a list of texts.\n",
    "        Returns a dictionary with 'input_ids' and 'attention_mask'.\n",
    "        \"\"\"\n",
    "        return self.tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16023564-c53c-419f-95a5-3dc8957b8667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "247a6d4d2f624d2197d6f354871aad99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fa4b40c3b8845ff9cfc247f13fb1e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.02k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b8b58c4797e4fae81da0bd28d87f6b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/222k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "925acc7e0a1441bbac10491ba84ce687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataFrame:\n",
      "                           text       label\n",
      "4   Doc 5 about marketing again   Marketing\n",
      "2  Doc 3 about technology again  Technology\n",
      "0        Doc 1 about technology  Technology\n",
      "3         Doc 4 about marketing   Marketing\n",
      "\n",
      "Validation DataFrame:\n",
      "                  text      label\n",
      "1  Doc 2 about finance  Financial\n",
      "\n",
      "Keys in tokenized output: dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "Shape of input_ids: torch.Size([4, 7])\n"
     ]
    }
   ],
   "source": [
    "test_data = [\n",
    "    {\"text\": \"Doc 1 about technology\", \"label\": \"Technology\"},\n",
    "    {\"text\": \"Doc 2 about finance\", \"label\": \"Financial\"},\n",
    "    {\"text\": \"Doc 3 about technology again\", \"label\": \"Technology\"},\n",
    "    {\"text\": \"Doc 4 about marketing\", \"label\": \"Marketing\"},\n",
    "    {\"text\": \"Doc 5 about marketing again\", \"label\": \"Marketing\"}  # Additional sample\n",
    "]\n",
    "\n",
    "prep = DataPreparator(model_name=\"nlpaueb/legal-bert-base-uncased\", max_length=32)\n",
    "train_df, val_df = prep.split_data(test_data, label_col='label')\n",
    "\n",
    "print(\"Train DataFrame:\")\n",
    "print(train_df)\n",
    "print(\"\\nValidation DataFrame:\")\n",
    "print(val_df)\n",
    "\n",
    "tokenized = prep.tokenize(train_df['text'].tolist())\n",
    "print(\"\\nKeys in tokenized output:\", tokenized.keys())\n",
    "print(\"Shape of input_ids:\", tokenized['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "451a838c-3693-4498-b1c9-47b036328284",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple classification model on top of a BERT base.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str, num_labels: int):\n",
    "        \"\"\"\n",
    "        :param model_name: The name of the pretrained model (FinBERT, LegalBERT, etc.).\n",
    "        :param num_labels: The number of possible labels/classes.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        \"\"\"\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output  # shape: (batch_size, hidden_size)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        output_dict = {\"logits\": logits}\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "            output_dict[\"loss\"] = loss\n",
    "\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4243ddb6-366a-4142-9495-1f85258c5577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46e79fb4b776422aabde2a9ac6839582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output keys: dict_keys(['logits', 'loss'])\n",
      "Logits shape: torch.Size([2, 3])\n",
      "Loss shape: torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "# We test a simple forward pass to ensure it works\n",
    "model = BertClassifier(model_name=\"nlpaueb/legal-bert-base-uncased\", num_labels=3)\n",
    "\n",
    "# Dummy input\n",
    "batch_size = 2\n",
    "seq_length = 8\n",
    "dummy_input_ids = torch.randint(0, 1000, (batch_size, seq_length))\n",
    "dummy_attention_mask = torch.ones((batch_size, seq_length))\n",
    "dummy_labels = torch.tensor([0, 1])  # some label IDs\n",
    "\n",
    "outputs = model(dummy_input_ids, attention_mask=dummy_attention_mask, labels=dummy_labels)\n",
    "print(\"Output keys:\", outputs.keys())\n",
    "print(\"Logits shape:\", outputs[\"logits\"].shape)\n",
    "print(\"Loss shape:\", outputs[\"loss\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "232725e8-694b-4672-ab2d-d8cfdea28ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for our documents.\n",
    "    Expects 'input_ids', 'attention_mask', and integer 'labels'.\n",
    "    \"\"\"\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def train_model(model, train_dataset, val_dataset, epochs=2, batch_size=4, lr=1e-5):\n",
    "    \"\"\"\n",
    "    Custom training loop for the classification model.\n",
    "    \"\"\"\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps=0,\n",
    "                                                num_training_steps=total_steps)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask, labels=labels)\n",
    "            loss = outputs[\"loss\"]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}, Training loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask, labels=labels)\n",
    "                val_loss += outputs[\"loss\"].item()\n",
    "\n",
    "                logits = outputs[\"logits\"]\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        accuracy = correct / total\n",
    "        print(f\"Validation loss: {avg_val_loss:.4f}, Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cedb7f4-9fa8-42bd-a6d0-9879d7499b38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading data/Non-Disclosure Agreements (NDA)/.DS_Store: 'utf-8' codec can't decode byte 0x80 in position 3131: invalid start byte\n",
      "Error reading data/Partnerships/.DS_Store: 'utf-8' codec can't decode byte 0x80 in position 3131: invalid start byte\n",
      "Unique labels found: ['Employment', 'Loans', 'Non-Disclosure Agreements (NDA)', 'Partnerships']\n",
      "Label2ID map: {'Employment': 0, 'Loans': 1, 'Non-Disclosure Agreements (NDA)': 2, 'Partnerships': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaron/miniconda3/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/2: 100%|████████████████████████████████████| 5/5 [00:02<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss: 1.5561\n",
      "Validation loss: 1.2840, Validation Accuracy: 0.6667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|████████████████████████████████████| 5/5 [00:02<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Training loss: 1.2940\n",
      "Validation loss: 1.2620, Validation Accuracy: 0.6667\n",
      "Model training complete and saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# List of folders to iterate through\n",
    "folders = [\n",
    "    \"data/Loans\",\n",
    "    \"data/Non-Disclosure Agreements (NDA)\",\n",
    "    \"data/Employment\",\n",
    "    \"data/Partnerships\"\n",
    "]\n",
    "\n",
    "# The label is derived from the folder names\n",
    "file_to_label = {}\n",
    "for folder in folders:\n",
    "    label = os.path.basename(folder)  # Use folder name as label\n",
    "    for filename in os.listdir(folder):\n",
    "        file_to_label[os.path.join(folder, filename)] = label\n",
    "\n",
    "# List to store data entries\n",
    "data_entries = []\n",
    "\n",
    "# Process each file in the mapped file_to_label\n",
    "for file_path, label in file_to_label.items():\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text_content = f.read()\n",
    "        data_entries.append({\"text\": text_content, \"label\": label})\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Now we have data_entries = [{ \"text\": \"...\", \"label\": \"...\" }, ...]\n",
    "# Build a label-to-ID map from the unique labels\n",
    "unique_labels = sorted(list(set(file_to_label.values())))\n",
    "label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "print(\"Unique labels found:\", unique_labels)\n",
    "print(\"Label2ID map:\", label2id)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Prepare data, split, tokenize, create Datasets\n",
    "model_name = \"nlpaueb/legal-bert-base-uncased\"\n",
    "prep = DataPreparator(model_name=model_name, max_length=128)\n",
    "\n",
    "train_df, val_df = prep.split_data(data_entries, label_col=\"label\", test_size=0.2)\n",
    "train_labels = train_df[\"label\"].map(label2id).tolist()\n",
    "val_labels = val_df[\"label\"].map(label2id).tolist()\n",
    "\n",
    "train_encodings = prep.tokenize(train_df[\"text\"].tolist())\n",
    "val_encodings = prep.tokenize(val_df[\"text\"].tolist())\n",
    "\n",
    "train_dataset = DocumentDataset(train_encodings, train_labels)\n",
    "val_dataset = DocumentDataset(val_encodings, val_labels)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Create and train the model\n",
    "num_labels = len(unique_labels)\n",
    "model = BertClassifier(model_name, num_labels=num_labels)\n",
    "\n",
    "trained_model = train_model(\n",
    "    model, \n",
    "    train_dataset, \n",
    "    val_dataset, \n",
    "    epochs=2, \n",
    "    batch_size=2, \n",
    "    lr=2e-5\n",
    ")\n",
    "\n",
    "# Save the fine-tuned model (optional)\n",
    "torch.save(trained_model.state_dict(), \"finbert_classifier.pt\")\n",
    "print(\"Model training complete and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fb06225-4425-41bb-af2b-731fcf334b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label for 'data/k-Non-Project-AC-1st-party-LOAN-AGREEMENT-LL-CL-DCL_Jan24_cleaned.txt': Employment\n"
     ]
    }
   ],
   "source": [
    "def predict_topic(trained_model, text, tokenizer, label2id_dict):\n",
    "    \"\"\"\n",
    "    Predict the topic for a single text using the trained model.\n",
    "    Returns the predicted label string.\n",
    "    \"\"\"\n",
    "    trained_model.eval()\n",
    "    inputs = tokenizer([text], padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = trained_model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs['logits']\n",
    "        preds = torch.argmax(logits, dim=1).cpu().item()\n",
    "\n",
    "    # Reverse lookup: find label string from predicted ID\n",
    "    id2label_local = {v: k for k, v in label2id_dict.items()}\n",
    "    return id2label_local[preds]\n",
    "\n",
    "\n",
    "# Example usage with some unlabeled text file\n",
    "unlabeled_file = \"data/k-Non-Project-AC-1st-party-LOAN-AGREEMENT-LL-CL-DCL_Jan24_cleaned.txt\"\n",
    "with open(unlabeled_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    text_content = f.read()\n",
    "\n",
    "predicted_label = predict_topic(trained_model, text_content, prep.tokenizer, label2id)\n",
    "print(f\"Predicted label for '{unlabeled_file}': {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8b9ef82-a3dd-4d7d-b4d0-792323162e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted topic: Employment\n"
     ]
    }
   ],
   "source": [
    "predicted = predict_topic(trained_model, text_content, prep.tokenizer, label2id)\n",
    "print(\"Predicted topic:\", predicted)\n",
    "# Compare 'predicted' with your known label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46908974-c21b-412d-af95-f2ce67095a85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
