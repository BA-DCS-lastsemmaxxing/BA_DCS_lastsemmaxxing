{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32e9a01b-2987-49a3-aed2-410f72be30aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModel, AdamW, get_linear_schedule_with_warmup\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Running on device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "accb7fc6-e885-4f24-b383-f9e190f475e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "class DataPreparator:\n",
    "    def __init__(self, model_name: str, max_length: int = 256):\n",
    "        self.model_name = model_name\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def split_data(self, data, label_col='label', text_col='text', test_size=0.2, random_state=42):\n",
    "        \"\"\"\n",
    "        Splits data into train and validation sets. Avoids stratification if class counts are too low.\n",
    "        \"\"\"\n",
    "        df = pd.DataFrame(data)\n",
    "        stratify_col = None\n",
    "\n",
    "        # Check if stratification is possible\n",
    "        if df[label_col].value_counts().min() > 1:\n",
    "            stratify_col = df[label_col]\n",
    "\n",
    "        train_df, val_df = train_test_split(\n",
    "            df, \n",
    "            test_size=test_size, \n",
    "            random_state=random_state, \n",
    "            stratify=stratify_col\n",
    "        )\n",
    "        return train_df, val_df\n",
    "    def tokenize(self, texts):\n",
    "        \"\"\"\n",
    "        Applies the BERT tokenizer to a list of texts.\n",
    "        Returns a dictionary with 'input_ids' and 'attention_mask'.\n",
    "        \"\"\"\n",
    "        return self.tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16023564-c53c-419f-95a5-3dc8957b8667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataFrame:\n",
      "                           text       label\n",
      "4   Doc 5 about marketing again   Marketing\n",
      "2  Doc 3 about technology again  Technology\n",
      "0        Doc 1 about technology  Technology\n",
      "3         Doc 4 about marketing   Marketing\n",
      "\n",
      "Validation DataFrame:\n",
      "                  text      label\n",
      "1  Doc 2 about finance  Financial\n",
      "\n",
      "Keys in tokenized output: dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "Shape of input_ids: torch.Size([4, 7])\n"
     ]
    }
   ],
   "source": [
    "test_data = [\n",
    "    {\"text\": \"Doc 1 about technology\", \"label\": \"Technology\"},\n",
    "    {\"text\": \"Doc 2 about finance\", \"label\": \"Financial\"},\n",
    "    {\"text\": \"Doc 3 about technology again\", \"label\": \"Technology\"},\n",
    "    {\"text\": \"Doc 4 about marketing\", \"label\": \"Marketing\"},\n",
    "    {\"text\": \"Doc 5 about marketing again\", \"label\": \"Marketing\"}  # Additional sample\n",
    "]\n",
    "\n",
    "prep = DataPreparator(model_name=\"ProsusAI/finbert\", max_length=32)\n",
    "train_df, val_df = prep.split_data(test_data, label_col='label')\n",
    "\n",
    "print(\"Train DataFrame:\")\n",
    "print(train_df)\n",
    "print(\"\\nValidation DataFrame:\")\n",
    "print(val_df)\n",
    "\n",
    "tokenized = prep.tokenize(train_df['text'].tolist())\n",
    "print(\"\\nKeys in tokenized output:\", tokenized.keys())\n",
    "print(\"Shape of input_ids:\", tokenized['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "451a838c-3693-4498-b1c9-47b036328284",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple classification model on top of a BERT base.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str, num_labels: int):\n",
    "        \"\"\"\n",
    "        :param model_name: The name of the pretrained model (FinBERT, LegalBERT, etc.).\n",
    "        :param num_labels: The number of possible labels/classes.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(model_name)\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "        \"\"\"\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output  # shape: (batch_size, hidden_size)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        output_dict = {\"logits\": logits}\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, labels)\n",
    "            output_dict[\"loss\"] = loss\n",
    "\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4243ddb6-366a-4142-9495-1f85258c5577",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe0e497e37a4da39e3ba6105b326de5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output keys: dict_keys(['logits', 'loss'])\n",
      "Logits shape: torch.Size([2, 3])\n",
      "Loss shape: torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "# We test a simple forward pass to ensure it works\n",
    "model = BertClassifier(model_name=\"ProsusAI/finbert\", num_labels=3)\n",
    "\n",
    "# Dummy input\n",
    "batch_size = 2\n",
    "seq_length = 8\n",
    "dummy_input_ids = torch.randint(0, 1000, (batch_size, seq_length))\n",
    "dummy_attention_mask = torch.ones((batch_size, seq_length))\n",
    "dummy_labels = torch.tensor([0, 1])  # some label IDs\n",
    "\n",
    "outputs = model(dummy_input_ids, attention_mask=dummy_attention_mask, labels=dummy_labels)\n",
    "print(\"Output keys:\", outputs.keys())\n",
    "print(\"Logits shape:\", outputs[\"logits\"].shape)\n",
    "print(\"Loss shape:\", outputs[\"loss\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "232725e8-694b-4672-ab2d-d8cfdea28ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for our documents.\n",
    "    Expects 'input_ids', 'attention_mask', and integer 'labels'.\n",
    "    \"\"\"\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def train_model(model, train_dataset, val_dataset, epochs=2, batch_size=4, lr=1e-5):\n",
    "    \"\"\"\n",
    "    Custom training loop for the classification model.\n",
    "    \"\"\"\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps=0,\n",
    "                                                num_training_steps=total_steps)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask, labels=labels)\n",
    "            loss = outputs[\"loss\"]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}, Training loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask, labels=labels)\n",
    "                val_loss += outputs[\"loss\"].item()\n",
    "\n",
    "                logits = outputs[\"logits\"]\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        accuracy = correct / total\n",
    "        print(f\"Validation loss: {avg_val_loss:.4f}, Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    return model, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cedb7f4-9fa8-42bd-a6d0-9879d7499b38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels found: ['Anti Money Laundering', 'Consumer Finance', 'Risk Management']\n",
      "Label2ID map: {'Anti Money Laundering': 0, 'Consumer Finance': 1, 'Risk Management': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaron/miniconda3/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/2: 100%|██████████████████████████████████| 5/5 [00:03<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss: 1.3175\n",
      "Validation loss: 1.1174, Validation Accuracy: 0.3333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|██████████████████████████████████| 5/5 [00:03<00:00,  1.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Training loss: 1.0808\n",
      "Validation loss: 1.0756, Validation Accuracy: 0.6667\n",
      "Model training complete and saved.\n"
     ]
    }
   ],
   "source": [
    "# replace manual labeling with csv file\n",
    "\n",
    "txt_folder = \"data\"\n",
    "file_to_label = {\n",
    "    \"acip-best-practices-for-the-management-of-ml-tf-and-pf-risks-from-customer-relationships-with-a-nexus-to-digital-assets_100723-(publish).txt\": \"Anti Money Laundering\",\n",
    "    \"BCBS Guidelines for AMLCFT June 2017.txt\": \"Anti Money Laundering\",\n",
    "    \"best-practices-for-countering-trade-based-money-laundering.txt\": \"Anti Money Laundering\",\n",
    "    \"Circular - Non-Face-to-Face Customer Due Diligence Measures-1.txt\":\"Anti Money Laundering\",\n",
    "    \"MAS 1106A 2023_04_cleaned.txt\":\"Consumer Finance\",\n",
    "    \"MAS Notice 1115_TDSR_290922_cleaned.txt\":\"Consumer Finance\",\n",
    "    \"Notice 1107 Bridging Loans for the Purchase of Immovable Properties_1 Jul 2021_cleaned.txt\":\"Consumer Finance\",\n",
    "    \"Notice 1109 Unsecured Credit Facilities to Individuals_1 Jul 2021_cleaned.txt\":\"Consumer Finance\",\n",
    "    \"TRM Guidelines 18 January 2021_cleaned.txt\":\"Risk Management\",\n",
    "    \"Response to Consultation Paper on Management of Outsourced Relevant Services_cleaned.txt\":\"Risk Management\",\n",
    "    \"Outsourcing Guidelines_Jul 2016 revised on 5 Oct 2018_cleaned.txt\":\"Risk Management\",\n",
    "    \"MAS Notices 644 655 644A 655A 1114 1118 Cancellation 2024_cleaned.txt\":\"Risk Management\",\n",
    "}\n",
    "\n",
    "# We'll read each file, store the text, and the known label\n",
    "data_entries = []\n",
    "for filename, label in file_to_label.items():\n",
    "    file_path = os.path.join(txt_folder, filename)\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text_content = f.read()\n",
    "    data_entries.append({\"text\": text_content, \"label\": label})\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Now we have data_entries = [{ \"text\": \"...\", \"label\": \"Financial\" }, ...]\n",
    "# We'll build a label2id map from the unique labels\n",
    "unique_labels = sorted(list(set(file_to_label.values())))\n",
    "label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "print(\"Unique labels found:\", unique_labels)\n",
    "print(\"Label2ID map:\", label2id)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Prepare data, split, tokenize, create Datasets\n",
    "model_name = \"ProsusAI/finbert\"\n",
    "prep = DataPreparator(model_name=model_name, max_length=128)\n",
    "\n",
    "train_df, val_df = prep.split_data(data_entries, label_col=\"label\", test_size=0.2)\n",
    "train_labels = train_df[\"label\"].map(label2id).tolist()\n",
    "val_labels = val_df[\"label\"].map(label2id).tolist()\n",
    "\n",
    "train_encodings = prep.tokenize(train_df[\"text\"].tolist())\n",
    "val_encodings = prep.tokenize(val_df[\"text\"].tolist())\n",
    "\n",
    "train_dataset = DocumentDataset(train_encodings, train_labels)\n",
    "val_dataset = DocumentDataset(val_encodings, val_labels)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Create and train the model\n",
    "num_labels = len(unique_labels)\n",
    "model = BertClassifier(model_name, num_labels=num_labels)\n",
    "\n",
    "trained_model = train_model(\n",
    "    model, \n",
    "    train_dataset, \n",
    "    val_dataset, \n",
    "    epochs=2, \n",
    "    batch_size=2, \n",
    "    lr=2e-5\n",
    ")\n",
    "\n",
    "# Save the fine-tuned model (optional)\n",
    "torch.save(trained_model.state_dict(), \"finbert_classifier.pt\")\n",
    "print(\"Model training complete and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0fb06225-4425-41bb-af2b-731fcf334b72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label for 'data/Guidelines on Outsourcing Banks_cleaned.txt': Risk Management\n"
     ]
    }
   ],
   "source": [
    "def predict_topic(trained_model, text, tokenizer, label2id_dict):\n",
    "    \"\"\"\n",
    "    Predict the topic for a single text using the trained model.\n",
    "    Returns the predicted label string.\n",
    "    \"\"\"\n",
    "    trained_model.eval()\n",
    "    inputs = tokenizer([text], padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = trained_model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs['logits']\n",
    "        preds = torch.argmax(logits, dim=1).cpu().item()\n",
    "\n",
    "    # Reverse lookup: find label string from predicted ID\n",
    "    id2label_local = {v: k for k, v in label2id_dict.items()}\n",
    "    return id2label_local[preds]\n",
    "\n",
    "\n",
    "# Example usage with some unlabeled text file\n",
    "unlabeled_file = \"data/Guidelines on Outsourcing Banks_cleaned.txt\"\n",
    "with open(unlabeled_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    text_content = f.read()\n",
    "\n",
    "predicted_label = predict_topic(trained_model, text_content, prep.tokenizer, label2id)\n",
    "print(f\"Predicted label for '{unlabeled_file}': {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8b9ef82-a3dd-4d7d-b4d0-792323162e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted topic: Risk Management\n"
     ]
    }
   ],
   "source": [
    "predicted = predict_topic(trained_model, text_content, prep.tokenizer, label2id)\n",
    "print(\"Predicted topic:\", predicted)\n",
    "# Compare 'predicted' with your known label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9652e893",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "531b2dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for our documents.\n",
    "    Expects 'input_ids', 'attention_mask', and integer 'labels'.\n",
    "    \"\"\"\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "def train_model(model, train_dataset, val_dataset, epochs=2, batch_size=4, lr=1e-5):\n",
    "    \"\"\"\n",
    "    Custom training loop for the classification model.\n",
    "    \"\"\"\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    total_steps = len(train_loader) * epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                                num_warmup_steps=0,\n",
    "                                                num_training_steps=total_steps)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_ids, attention_mask, labels=labels)\n",
    "            loss = outputs[\"loss\"]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}, Training loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(input_ids, attention_mask, labels=labels)\n",
    "                val_loss += outputs[\"loss\"].item()\n",
    "\n",
    "                logits = outputs[\"logits\"]\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "                correct += (preds == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        accuracy = correct / total\n",
    "        print(f\"Validation loss: {avg_val_loss:.4f}, Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    return model, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46908974-c21b-412d-af95-f2ce67095a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels found: ['Anti Money Laundering', 'Consumer Finance', 'Risk Management']\n",
      "Label2ID map: {'Anti Money Laundering': 0, 'Consumer Finance': 1, 'Risk Management': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "Epoch 1/2: 100%|██████████| 19/19 [00:08<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training loss: 1.0273\n",
      "Validation loss: 0.9999, Validation Accuracy: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|██████████| 19/19 [00:07<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Training loss: 0.9252\n",
      "Validation loss: 0.9727, Validation Accuracy: 0.5000\n",
      "Model training complete and saved.\n"
     ]
    }
   ],
   "source": [
    "# replace manual labeling with csv file\n",
    "\n",
    "txt_folder = \"cleaned_data_for_testing\"\n",
    "file_to_label = {\n",
    "    \"acip-best-practices-for-the-management-of-ml-tf-and-pf-risks-from-customer-relationships-with-a-nexus-to-digital-assets_100723-(publish)_cleaned.txt\": \"Anti Money Laundering\",\n",
    "    \"BCBS Guidelines for AMLCFT June 2017_cleaned.txt\": \"Anti Money Laundering\",\n",
    "    \"best-practices-for-countering-trade-based-money-laundering_cleaned.txt\": \"Anti Money Laundering\",\n",
    "    \"Circular on MyInfo and CDD on NFTF business relations_cleaned.txt\":\"Anti Money Laundering\",\n",
    "    \"Effective Practices to Detect and Mitigate the Risk from Misuse of Legal Persons June 2019_cleaned.txt\":\"Anti Money Laundering\",\n",
    "    \"Guidelines for Financial Institutions to Safeguard the Integrity of Singapores Financial System  Sep 2011_cleaned.txt\":\"Anti Money Laundering\",\n",
    "    \"Guidelines to MAS Notice 1014  November 2015_cleaned.txt\":\"Anti Money Laundering\",\n",
    "    \"Guidelines to PSN01 dated 2 April 2024_cleaned.txt\":\"Anti Money Laundering\",\n",
    "    \"Industry Perspectives  Adopting Data Analytics Methods for AMLCFT_cleaned.txt\":\"Anti Money Laundering\",\n",
    "    \"Infographic on Effective AMLCFT Transaction Monitoring Controls_cleaned.txt\":\"Anti Money Laundering\",\n",
    "    \"MAS Notice 1014 last revised on 1 March 2022_cleaned.txt\":\"Anti Money Laundering\",\n",
    "    \"MAS Notice PSN10 - Exempt Payment Service Providers_cleaned.txt\":\"Anti Money Laundering\",\n",
    "    \"Notice PSM-N01 dated 1 March 2022-1_cleaned.txt\":\"Anti Money Laundering\",\n",
    "\n",
    "\n",
    "    \"2021-06-28 MAS Notice 1106_cleaned.txt\":\"Consumer Finance\",\n",
    "    \"2021-06-28 Notice 1106B_COVID_LTV_cleaned.txt\":\"Consumer Finance\",\n",
    "    \"2021-06-28 Notice 1115A_COVID_TDSR_cleaned.txt\":\"Consumer Finance\",\n",
    "    \"Compliance Toolkit for Merchant Banks Last Revised 12 September 2024_cleaned.txt\":\"Consumer Finance\",\n",
    "    \"MAS 1106A 2023_04_cleaned.txt\":\"Consumer Finance\",\n",
    "    \"MAS Notice 1113 Motor Vehicle Loans - Merchant Bank_1 July 2021_cleaned.txt\":\"Consumer Finance\",\n",
    "    \"MAS Notice 1115_TDSR_290922_cleaned.txt\":\"Consumer Finance\",\n",
    "    \"Notice 1107 Bridging Loans for the Purchase of Immovable Properties_1 Jul 2021_cleaned.txt\":\"Consumer Finance\",\n",
    "    \"Notice 1109 Unsecured Credit Facilities to Individuals_1 Jul 2021_cleaned.txt\":\"Consumer Finance\",\n",
    "\n",
    "    \"7 Credit Facilities to Directors Related Corporations etc 01 Nov 1985_cleaned.txt\":\"Risk Management\",\n",
    "    \"8 Credit Facilities and Limits 01 Nov 1985_cleaned.txt\":\"Risk Management\",\n",
    "    \"BCM-Guidelines-June-2022_cleaned.txt\":\"Risk Management\",\n",
    "    \"blue_book_wholesale_cleaned.txt\":\"Risk Management\",\n",
    "    \"Board-and-Senior-Mgmt_01 Jul 2021_cleaned.txt\":\"Risk Management\",\n",
    "    \"Compliance Toolkit for Merchant Banks Last Revised 12 September 2024_cleaned.txt\":\"Risk Management\",\n",
    "    \"Consultation Paper on Guidelines on Transition Planning Banks_cleaned.txt\":\"Risk Management\",\n",
    "    \"Directive 5_cleaned.txt\":\"Risk Management\",\n",
    "    \"FAQ - Notice on Cyber Hygiene_cleaned.txt\":\"Risk Management\",\n",
    "    \"FAQ - Notice on Technology Risk Management_cleaned.txt\":\"Risk Management\",\n",
    "    \"FAQ for Notice 658 and 1121_11Dec2024_cleaned.txt\":\"Risk Management\",\n",
    "    \"Guidelines on Definition of a Deposit_01 Jul 2021_cleaned.txt\":\"Risk Management\",\n",
    "    \"Guidelines on Environmental Risk Management for Banks_cleaned.txt\":\"Risk Management\",\n",
    "    \"Guidelines on Outsourcing Banks_cleaned.txt\":\"Risk Management\",\n",
    "    \"Guidelines on Risk Management Practices  Internal Controls July 2024_cleaned.txt\":\"Risk Management\",\n",
    "    \"Information Paper on Environmental Risk Management Banks_cleaned.txt\":\"Risk Management\",\n",
    "    \"MAS NOTICE 1004_11062021_cleaned.txt\":\"Risk Management\",\n",
    "    \"MAS Notice 1005_29 Jun 2021_cleaned.txt\":\"Risk Management\",\n",
    "    \"MAS Notice 1005A_30 Jun 2021_cleaned.txt\":\"Risk Management\",\n",
    "    \"MAS Notice 1015 effective 01 July 2024_cleaned.txt\":\"Risk Management\",\n",
    "    \"MAS Notice 1108 Cancellation_cleaned.txt\":\"Risk Management\",\n",
    "    \"MAS Notice 1121 - 4_cleaned.txt\":\"Risk Management\",\n",
    "    \"MAS Notice FSM-N11_cleaned.txt\":\"Risk Management\",\n",
    "    \"MAS Notices 644 655 644A 655A 1114 1118 Cancellation 2024_cleaned.txt\":\"Risk Management\",\n",
    "    \"Outsourcing Guidelines_Jul 2016 revised on 5 Oct 2018_cleaned.txt\":\"Risk Management\",\n",
    "    \"Response to Consultation Paper on Management of Outsourced Relevant Services_cleaned.txt\":\"Risk Management\",\n",
    "    \"TRM Guidelines 18 January 2021_cleaned.txt\":\"Risk Management\", \n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "# We'll read each file, store the text, and the known label\n",
    "data_entries = []\n",
    "for filename, label in file_to_label.items():\n",
    "    file_path = os.path.join(txt_folder, filename)\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text_content = f.read()\n",
    "    data_entries.append({\"text\": text_content, \"label\": label})\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Now we have data_entries = [{ \"text\": \"...\", \"label\": \"Financial\" }, ...]\n",
    "# We'll build a label2id map from the unique labels\n",
    "unique_labels = sorted(list(set(file_to_label.values())))\n",
    "label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "id2label = {v: k for k, v in label2id.items()}\n",
    "\n",
    "print(\"Unique labels found:\", unique_labels)\n",
    "print(\"Label2ID map:\", label2id)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Prepare data, split, tokenize, create Datasets\n",
    "model_name = \"ProsusAI/finbert\"\n",
    "prep = DataPreparator(model_name=model_name, max_length=128)\n",
    "\n",
    "train_df, val_df = prep.split_data(data_entries, label_col=\"label\", test_size=0.2)\n",
    "train_labels = train_df[\"label\"].map(label2id).tolist()\n",
    "val_labels = val_df[\"label\"].map(label2id).tolist()\n",
    "\n",
    "train_encodings = prep.tokenize(train_df[\"text\"].tolist())\n",
    "val_encodings = prep.tokenize(val_df[\"text\"].tolist())\n",
    "\n",
    "train_dataset = DocumentDataset(train_encodings, train_labels)\n",
    "val_dataset = DocumentDataset(val_encodings, val_labels)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# Create and train the model\n",
    "num_labels = len(unique_labels)\n",
    "model = BertClassifier(model_name, num_labels=num_labels)\n",
    "\n",
    "trained_model, val_loader = train_model(\n",
    "    model, \n",
    "    train_dataset, \n",
    "    val_dataset, \n",
    "    epochs=2, \n",
    "    batch_size=2, \n",
    "    lr=2e-5\n",
    ")\n",
    "\n",
    "# Save the fine-tuned model (optional)\n",
    "torch.save(trained_model.state_dict(), \"finbert_classifier.pt\")\n",
    "print(\"Model training complete and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72432c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5\n",
      "Precision: 0.25\n",
      "Recall: 0.5\n",
      "F1 Score: 0.3333333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the validation set\n",
    "trained_model.eval()\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "for batch in val_loader:\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = trained_model(input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs[\"logits\"]\n",
    "    predictions.extend(torch.argmax(logits, dim=-1).cpu().numpy())\n",
    "    true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9523f0d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "1c74280073e0e2d1b1a443f530ccd6f3fc15affc33bce9394f7c3af6eb4cd51b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
